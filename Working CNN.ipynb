{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import datasets, transforms, utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.models as models\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CNN for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Define CNN\n",
    "learning_rate = 1e-4\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "\n",
    "    \n",
    "    #Define convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding = 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, padding = 1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=5, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=5, padding=1)\n",
    "\n",
    "    #Define pooling layers\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=5, stride = 2)\n",
    "\n",
    "\n",
    "    #Define fully connected layers\n",
    "        self.fc1 = nn.Linear(512*2*2,512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256,128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 1)\n",
    "\n",
    "    #Dropout some neurons to prevent overfitting.\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    #Define activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity = nn.Identity()\n",
    "\n",
    "\n",
    "#Apply convolutional layers with pooling in between\n",
    "    def forward(self, x):\n",
    "        feature_map = []\n",
    "        x = self.max_pool(nn.functional.relu(self.conv1(x)))\n",
    "        feature_map.append(x)\n",
    "        x = self.max_pool(nn.functional.relu(self.conv2(x)))\n",
    "        feature_map.append(x)\n",
    "        x = self.max_pool(nn.functional.relu(self.conv3(x)))\n",
    "        feature_map.append(x)\n",
    "        x = self.max_pool(nn.functional.relu(self.conv4(x)))\n",
    "        feature_map.append(x)\n",
    "        x = self.max_pool(nn.functional.relu(self.conv5(x)))\n",
    "        feature_map.append(x)\n",
    "        \n",
    "        \n",
    "\n",
    "#Flatten output\n",
    "        x = x.view(-1, 512*2*2)\n",
    "\n",
    "        x = self.dropout(nn.functional.relu(self.fc1(x)))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu((self.fc4(x)))\n",
    "        x = self.identity(self.fc5(x))\n",
    "        \n",
    "\n",
    "        return x\n",
    "\n",
    "model = Model()\n",
    "device = torch.device('cpu')  # use cuda or cpu\n",
    "model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"C:/Users/sebas/OneDrive/Dokumenter/skole/4 Semester/Fagprojekt/Data/RedditDataWithLinks.csv\"\n",
    "posts_tidy_df = pd.read_csv(data_path)\n",
    "pd.set_option('display.max_columns', None)\n",
    "posts_tidy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ImageWithScore(img_path, csv):\n",
    "    data_path = pd.read_csv(csv)\n",
    "    transformed_images = []\n",
    "    scores = []\n",
    "    corrupted = []\n",
    "    counter = 0\n",
    "\n",
    "    for index, row in data_path.iterrows():\n",
    "        if counter % 1000 == 0:\n",
    "            print('Current progress is at: {count}'.format(count=counter))\n",
    "\n",
    "        submission_id = row['PostID'] + \".jpg\"\n",
    "        score = np.log10(row['AppliedScale']) #Choose target here!!\n",
    "        image_path = os.path.join(img_path, submission_id)\n",
    "        image_path = os.path.join(image_path).replace(\"\\\\\", \"/\")\n",
    "        if os.path.exists(image_path):\n",
    "            try:\n",
    "                with Image.open(image_path) as image:\n",
    "                    # Normalize to mean and std of ImageNet\n",
    "                    transform = transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    ])\n",
    "\n",
    "                    transformed_image = transform(image)\n",
    "                    transformed_images.append(transformed_image)\n",
    "                    scores.append(score)\n",
    "                    counter += 1\n",
    "            except Exception as e:\n",
    "                print(f'Image failed: {submission_id}: {e}')\n",
    "                corrupted.append(submission_id)\n",
    "                counter += 1\n",
    "        else:\n",
    "            print(f'Image not found: {image_path}')\n",
    "            counter += 1\n",
    "\n",
    "    return transformed_images, scores\n",
    "\n",
    "image_path = \"C:/Users/sebas/OneDrive/Dokumenter/skole/4 Semester/Fagprojekt/Images/\"\n",
    "data_path = \"C:/Users/sebas/OneDrive/Dokumenter/skole/4 Semester/Fagprojekt/Data/RedditDataWithLinks.csv\"\n",
    "\n",
    "transformed_images, scores = ImageWithScore(image_path, data_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into test/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_images, test_images, train_scores, test_scores = train_test_split(transformed_images, scores, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data = list(zip(train_images, train_scores))\n",
    "test_data = list(zip(test_images, test_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(Loader(train_data), batch_size=10, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(Loader(test_data), batch_size=10, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data properly for pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Loader(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, score = self.data[index]\n",
    "        return image, score\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loop\n",
    "train_loader = torch.utils.data.DataLoader(Loader(train_data), batch_size=10, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(Loader(test_data), batch_size=10, shuffle=False)\n",
    "\n",
    "def CNN(learning_rate, batch_size, num_epochs):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for i, (images, score) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if images.dim() == 3:\n",
    "                images = torch.unsqueeze(images, dim=0)\n",
    "\n",
    "            output = model(images.float())\n",
    "            loss = criterion(output.float(), score.float()) \n",
    "            loss = loss.mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Test the model\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                if data.dim() == 3:\n",
    "                    data = torch.unsqueeze(data, dim=0)\n",
    "                output = model(data)\n",
    "                test_loss += criterion(output, target)\n",
    "\n",
    "            avg_test_loss = test_loss / len(test_loader)\n",
    "            test_losses.append(avg_test_loss)\n",
    "\n",
    "        print('Epoch [{}/{}], Train Loss: {:.4f}, Test Loss: {:.4f}'.format(epoch + 1, num_epochs, avg_train_loss, avg_test_loss))\n",
    "\n",
    "    print(\"Finished training.\")\n",
    "\n",
    "    return model, train_losses, test_losses\n",
    "\n",
    "model, train_losses, test_losses = CNN(learning_rate=1e-4, batch_size=10, num_epochs=15)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'C:/Users/sebas/OneDrive/Dokumenter/skole/4 Semester/Fagprojekt/Data/saved_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mean score\n",
    "train_scores = []\n",
    "for images, scores in train_loader:\n",
    "    train_scores.extend(scores.tolist())\n",
    "\n",
    "mean_score = sum(train_scores) / len(train_scores)\n",
    "print(\"Mean of train set scores:\", mean_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Median score\n",
    "train_scores = []\n",
    "for images, scores in train_loader:\n",
    "    train_scores.extend(scores.tolist())\n",
    "\n",
    "median_score = np.median(train_scores)\n",
    "print(\"Median of train set scores:\", median_score)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict scores (Our own)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and change last layer and evaluate\n",
    "model = Model()\n",
    "\n",
    "model.load_state_dict(torch.load('C:/Users/sebas/OneDrive/Dokumenter/skole/4 Semester/Fagprojekt/Data/saved_model.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "predicted_scores = []\n",
    "originals = []\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, scores in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        predicted_scores.extend(outputs.squeeze().cpu().numpy())\n",
    "\n",
    "        # Make tuples for sorting easily\n",
    "        original_tuples = list(zip(images, scores))\n",
    "        originals.extend(original_tuples)\n",
    "        predicted_tuples = list(zip(images, outputs.squeeze().cpu().numpy()))\n",
    "        predictions.extend(predicted_tuples)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert the predicted_scores list to a numpy array\n",
    "predicted_scores = np.array(predicted_scores)\n",
    "\n",
    "# Convert the target scores to a numpy array\n",
    "actual_scores = np.array(test_scores)\n",
    "\n",
    "# Plotting the predicted scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(range(len(predicted_scores)), predicted_scores, label='Predicted Scores')\n",
    "plt.xlabel('Image Index')\n",
    "plt.ylabel('Predicted Score')\n",
    "plt.title('Predicted Scores')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the actual scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(range(len(actual_scores)), actual_scores, label='Actual Scores')\n",
    "plt.xlabel('Image Index')\n",
    "plt.ylabel('Actual Score')\n",
    "plt.title('Actual Scores')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Images with highest and lowest score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inverse ImageNet normalisation\n",
    "normalise_inverse = transforms.Normalize(\n",
    "    mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],\n",
    "    std=[1 / 0.229, 1 / 0.224, 1 / 0.225]\n",
    ")\n",
    "\n",
    "sorted_original_data = sorted(originals, key=lambda x: x[1])\n",
    "sorted_predicted_data = sorted(predictions, key=lambda x: x[1])\n",
    "\n",
    "top_10_high_original = sorted_original_data[-10:]\n",
    "top_10_low_original = sorted_original_data[:10]\n",
    "\n",
    "top_10_high_predicted = sorted_predicted_data[-10:]\n",
    "top_10_low_predicted = sorted_predicted_data[:10]\n",
    "\n",
    "def plot_images_with_scores(images, scores, title):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    for i, (image, score) in enumerate(zip(images, scores)):\n",
    "        denormalized_image = normalise_inverse(image)\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(denormalized_image.permute(1, 2, 0).cpu().numpy())\n",
    "        plt.title(f\"Score: {score.item():.2f}\") \n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "predicted_scores = [score.item() for _, score in predictions]\n",
    "\n",
    "\n",
    "plot_images_with_scores([image for image, _ in top_10_high_original], [score for _, score in top_10_high_original], \"Top 10 Images with Highest Original Scores\")\n",
    "plot_images_with_scores([image for image, _ in top_10_low_original], [score for _, score in top_10_low_original], \"Top 10 Images with Lowest Original Scores\")\n",
    "plot_images_with_scores([image for image, _ in top_10_high_predicted], [score for _, score in top_10_high_predicted], \"Top 10 Images with Highest Predicted Scores\")\n",
    "plot_images_with_scores([image for image, _ in top_10_low_predicted], [score for _, score in top_10_low_predicted], \"Top 10 Images with Lowest Predicted Scores\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test model on actual top and bottom 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = [image for image, _ in top_10_high_original]\n",
    "\n",
    "predicted_scores_top_10 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for image in test_images:\n",
    "        image = image.to(device)\n",
    "        output = model(image.unsqueeze(0))\n",
    "        predicted_scores_top_10.append(output.item())\n",
    "\n",
    "print(\"Top 10 Actual and Predicted Scores:\")\n",
    "for i, (image, actual_score) in enumerate(top_10_high_original):\n",
    "    predicted_score = predicted_scores_top_10[i]\n",
    "    print(f\"Image {i+1}: Actual Score = {actual_score:.2f}, Predicted Score = {predicted_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = [image for image, _ in top_10_low_original]\n",
    "\n",
    "predicted_scores_bottom_10 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for image in test_images:\n",
    "        image = image.to(device)\n",
    "        output = model(image.unsqueeze(0))\n",
    "        predicted_scores_bottom_10.append(output.item())\n",
    "\n",
    "print(\"Bottom 10 Actual and Predicted Scores:\")\n",
    "for i, (image, actual_score) in enumerate(top_10_low_original):\n",
    "    predicted_score = predicted_scores_bottom_10[i]\n",
    "    print(f\"Image {i+1}: Actual Score = {actual_score:.2f}, Predicted Score = {predicted_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(actual_scores_top_10, predicted_scores_top_10, label='Top 10')\n",
    "plt.scatter(actual_scores_bottom_10, predicted_scores_bottom_10, label='Bottom 10')\n",
    "plt.xlabel('Actual Score')\n",
    "plt.ylabel('Predicted Score')\n",
    "plt.title('Actual Scores vs Predicted Scores')\n",
    "plt.xlim(y_min, y_max)\n",
    "plt.ylim(-1, y_max)\n",
    "plt.plot([y_min, y_max], [y_min, y_max], color='red', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_score = min(np.min(predicted_scores), np.min(actual_scores))\n",
    "max_score = max(np.max(predicted_scores), np.max(actual_scores))\n",
    "y_min, y_max = min_score * 1.1, max_score * 1.1 \n",
    "\n",
    "predicted_scores = np.array(predicted_scores)\n",
    "actual_scores = np.array(actual_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting actual scores and predicted scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(actual_scores, predicted_scores)\n",
    "plt.xlabel('Actual Score')\n",
    "plt.ylabel('Predicted Score')\n",
    "plt.title('Actual Scores vs Predicted Scores')\n",
    "plt.xlim(y_min, y_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.plot([y_min, y_max], [y_min, y_max], color='red', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize feature map "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract features (Lowest upvoted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "\n",
    "model.load_state_dict(torch.load('C:/Users/sebas/OneDrive/Dokumenter/skole/4 Semester/Fagprojekt/Data/saved_model.pth'))\n",
    "model.to(device)\n",
    "\n",
    "lowest_score_image = top_10_low_predicted[0][0]\n",
    "input_tensor = lowest_score_image.unsqueeze(0).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Visualize activation map for conv1\n",
    "    conv1_features = model.conv1(input_tensor)\n",
    "    conv1_activation_maps = conv1_features.squeeze(0).cpu().numpy()\n",
    "\n",
    "# Visualize the activation maps\n",
    "fig, axes = plt.subplots(nrows=1, ncols=8, figsize=(12, 2))\n",
    "\n",
    "# Visualize conv1 activation maps\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(conv1_activation_maps[i])\n",
    "    ax.axis('off')\n",
    "axes[0].set_title('conv1')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract features (Highest upvoted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "\n",
    "model.load_state_dict(torch.load('C:/Users/sebas/OneDrive/Dokumenter/skole/4 Semester/Fagprojekt/Data/saved_model.pth'))\n",
    "model.to(device)\n",
    "\n",
    "highest_score_image = top_10_high_predicted[-1][0]\n",
    "input_tensor = highest_score_image.unsqueeze(0).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Visualize activation map for conv1\n",
    "    conv1_features = model.conv1(input_tensor)\n",
    "    conv1_activation_maps = conv1_features.squeeze(0).cpu().numpy()\n",
    "\n",
    "# Visualize the activation maps\n",
    "fig, axes = plt.subplots(nrows=1, ncols=8, figsize=(12, 2))\n",
    "\n",
    "# Visualize conv1 activation maps\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(conv1_activation_maps[i])\n",
    "    ax.axis('off')\n",
    "axes[0].set_title('conv1')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saliency Map (Highest upvote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "\n",
    "model.load_state_dict(torch.load('C:/Users/sebas/OneDrive/Dokumenter/skole/4 Semester/Fagprojekt/Data/saved_model.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Define the input tensor\n",
    "input_tensor = torch.tensor(highest_score_image, device=device, dtype=torch.float32)\n",
    "input_tensor = normalise_inverse(input_tensor)\n",
    "input_tensor = input_tensor.unsqueeze(0).requires_grad_()\n",
    "\n",
    "output = model(input_tensor)\n",
    "\n",
    "model.zero_grad()\n",
    "output.backward()\n",
    "\n",
    "gradients = input_tensor.grad[0].detach().cpu()\n",
    "\n",
    "grayscale_gradients = np.abs(gradients.detach().cpu().numpy()).mean(axis=0)\n",
    "\n",
    "normalized_gradients = (grayscale_gradients - np.min(grayscale_gradients)) / (\n",
    "    np.max(grayscale_gradients) - np.min(grayscale_gradients)\n",
    ")\n",
    "\n",
    "# Plot the input image and the saliency map\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes[0].imshow(input_tensor.squeeze().permute(1, 2, 0).detach().cpu().numpy())\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Input Image')\n",
    "axes[1].imshow(normalized_gradients, cmap='hot')\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Saliency Map')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saliency Map (Lowest upvoted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the model and load the weights\n",
    "model = Model()\n",
    "\n",
    "model.load_state_dict(torch.load('C:/Users/sebas/OneDrive/Dokumenter/skole/4 Semester/Fagprojekt/Data/saved_model.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Define the input tensor\n",
    "input_tensor = torch.tensor(lowest_score_image, device=device, dtype=torch.float32)\n",
    "\n",
    "# Reverse normalization\n",
    "normalise_inverse = transforms.Normalize(\n",
    "    mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],\n",
    "    std=[1 / 0.229, 1 / 0.224, 1 / 0.225]\n",
    ")\n",
    "input_tensor = normalise_inverse(input_tensor)\n",
    "\n",
    "input_tensor = input_tensor.unsqueeze(0).requires_grad_()  # Create a new tensor with requires_grad=True\n",
    "\n",
    "output = model(input_tensor)\n",
    "\n",
    "model.zero_grad()\n",
    "output.backward()\n",
    "\n",
    "gradients = input_tensor.grad[0].detach().cpu()\n",
    "\n",
    "grayscale_gradients = np.abs(gradients.detach().cpu().numpy()).mean(axis=0)\n",
    "\n",
    "normalized_gradients = (grayscale_gradients - np.min(grayscale_gradients)) / (\n",
    "    np.max(grayscale_gradients) - np.min(grayscale_gradients)\n",
    ")\n",
    "\n",
    "# Plot the input image and the saliency map\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes[0].imshow(input_tensor.squeeze().permute(1, 2, 0).detach().cpu().numpy())\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Input Image')\n",
    "axes[1].imshow(normalized_gradients, cmap='hot')\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Saliency Map')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saliency map specific picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming you have the trained model saved as 'model.pth'\n",
    "model_path = 'C:/Users/sebas/OneDrive/Dokumenter/skole/4 Semester/Fagprojekt/Data/saved_model.pth'\n",
    "\n",
    "# Load the trained model\n",
    "model = Model()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "# Preprocess the input image\n",
    "input_image = Image.open(\"{}/{}\".format(image_path, \"h7rkz.jpg\"))\n",
    "input_image = input_image.resize((224, 224))\n",
    "input_tensor = transform(input_image).unsqueeze(0)\n",
    "\n",
    "# Set the model to evaluation mode and disable gradients\n",
    "model.eval()\n",
    "input_tensor.requires_grad_()\n",
    "\n",
    "# Forward pass to obtain the output\n",
    "output = model(input_tensor)\n",
    "\n",
    "# Calculate the gradients of the output with respect to the input\n",
    "output.backward()\n",
    "\n",
    "# Get the gradients from the input tensor\n",
    "gradients = input_tensor.grad[0]\n",
    "\n",
    "# Convert the gradients to grayscale\n",
    "grayscale_gradients = np.abs(gradients.numpy()).mean(axis=0)\n",
    "\n",
    "# Normalize the gradients\n",
    "normalized_gradients = (grayscale_gradients - np.min(grayscale_gradients)) / (\n",
    "    np.max(grayscale_gradients) - np.min(grayscale_gradients)\n",
    ")\n",
    "\n",
    "# Plot the original image and the saliency map\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes[0].imshow(input_image)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Input Image')\n",
    "axes[1].imshow(normalized_gradients, cmap='hot')\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Saliency Map')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
